{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Robust Algorithms for Online Allocation Problems Using Adversarial Training\n",
    "\n",
    "Author list:\n",
    "- Goran Zuzic\n",
    "- Di Wang\n",
    "- Aranyak Mehta\n",
    "- D. Sivakumar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "if device == torch.device('cuda'):\n",
    "    print(torch.cuda.get_device_name(device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YaoLearner framework\n",
    "\n",
    "adversarial training for finding max-min strategies througha training framework that we call **YaoLearner** due to the inspiration from Yao’s Lemma (Appendix C).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type definition\n",
    "Budgets = Tensor\n",
    "Bid = Tensor\n",
    "\n",
    "Input = tuple[Budgets, Bid]\n",
    "\n",
    "# Hyper Parameters\n",
    "# T, T_{alg}, T_{adv}, T_{add}, T_{restart}, n_{batch}\n",
    "n_batch = 100\n",
    "n_noice = 100\n",
    "T_alg = 4\n",
    "T_adv = 4\n",
    "\n",
    "# distribution D\n",
    "\n",
    "# parameter $\\alpha$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm network\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm network takes the problem-specific online request as input (e.g., **arriving bid** in the AdWords), and outputs an irrevocable decision (e.g., assignment of the bid in AdWords). \n",
    "\n",
    "Its weights start of at random and are trained until the learned algorithm converges. \n",
    "\n",
    "The adversary network takes random noise as input and outputs the full input (e.g., a complete sequence of bid weights). \n",
    "\n",
    "The weights of the adversary network are frequently reinitialized in order to maximize the probability of finding the worst-case input to a moving algorithm network. \n",
    "\n",
    "Naturally, these two networks require us to model the space of algorithms and inputs with the parameters of the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlgorithmNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(AlgorithmNet, self).__init__()\n",
    "        # TODO\n",
    "        \n",
    "    def run(i: Input) -> Tensor:\n",
    "        pass\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversary network\n",
    "\n",
    "The output of the adversary network is an instance of the AdWords problem, which consists of an $m×n$ matrix $A$. \n",
    "\n",
    "The $j,i$-th entry of $A$ is a real value in $[0,1]$ representing the bid of advertiser $i$ for the $j$-th arriving *ad-slot*. \n",
    "\n",
    "In most of our experiments we fix the budget of each advertiser to bem/n, butwe also have experiments (in robust stochastic training) where the adversary network additionally outputsann-dimensional vector $B$ of individual budgets.\n",
    "The adversary network takes as input an $m$-dimensional **random Gaussian vector** to provide randomness to the search for the difficult instances.\n",
    "\n",
    "\n",
    "Unlike the algorithm network, which is designed to work on arbitrarym,n, we do not need the adversarynetwork to scale: in our training, we fix $m= 25,n= 5$ for the adversary network. \n",
    "Since the role of theadversary network is to generate instances to train the algorithm network, we note that, in general, it isdesirable for it to generate fixed small-size instances (that capture the essential difficulty of the problem) sothat training time/resources may be minimized. \n",
    "One may also want to use the adversary-generated instancesto gain human-comprehensible intuition about the weakness of an algorithm, and again in this case small butnot tiny instances would best serve the purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversaryNet(nn.Module):\n",
    "    def __init__(self, m: int = 25, n: int = 5) -> None:\n",
    "        super(AdversaryNet, self).__init__()\n",
    "        # matrix A\n",
    "        # TODO\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        pass\n",
    "\n",
    "    def re_initialization(self) -> None:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An evaluation environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluates  the  algorithm  networkalgon  an  inputiand  assigns  to  it  acompetitive ratio $V(\\mathrm{ALG},i)$. We assume in this section that the algorithm wants to maximize the CR andthe adversary wants to minimize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OPT(i):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Alg is evaluates the algorithm network ALG on an input i\n",
    "def V(ALG: AlgorithmNet, i: Input) -> Tensor:\n",
    "    \n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An experience array\n",
    "\n",
    "**An experience array** contains the  historical best  responses generated  by  the adversary network. \n",
    "\n",
    "The adversary network goes through cycles of weight re-initialization, training until convergence and storing itsfinal result into the experience array. Moreover, the algorithm network is trained via the experience array and not via the adversary network directly. \n",
    "\n",
    "At each training step of the algorithm, we take some number of samples from the experience array, evaluate the algorithm’s performances by computing the $\\mathrm{CR}$, find the worst input (smallest $\\mathrm{CR}$) among them for the current algorithm, and train the algorithm against that input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E: list[Input] = []\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8e0e1919a95824482f7c7ed5b9775f8717706ba4fd6cb589a6d66ba4fe99650"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
